{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(path_to_json):\n",
    "    ####################\n",
    "    #print(path_to_json)\n",
    "    ####################\n",
    "    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "    #######################\n",
    "    #print(json_files[3:4])\n",
    "    #######################\n",
    "    json_objects = list()\n",
    "    for index, js in enumerate(json_files):\n",
    "        #######################\n",
    "        #print(index, \": \", js)\n",
    "        #######################\n",
    "        with open(os.path.join(path_to_json, js)) as json_file:\n",
    "            json_objects.append(json.load(json_file))\n",
    "    #####################\n",
    "    #pprint(json_objects)\n",
    "    #####################\n",
    "    \n",
    "    del json_objects[3]\n",
    "    del json_objects[28-1]\n",
    "    del json_objects[75-2]\n",
    "    del json_objects[114-3]\n",
    "    del json_objects[175-4]\n",
    "    del json_objects[238-5]\n",
    "    del json_objects[261-6]\n",
    "    del json_objects[279-7]\n",
    "    del json_objects[327-8]\n",
    "    del json_objects[364-9]\n",
    "    del json_objects[374-10]\n",
    "    del json_objects[402-11]\n",
    "    del json_objects[406-12]\n",
    "    \n",
    "    return json_objects#[:1]\n",
    "##### index number 3, 28, 75, 114, 175, 238, 261, 279, 327, 364, 374, 402, 406 has denotation_span = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = None\n",
    "with open(\"stopwords-en.txt\", \"rt\", encoding=\"utf-8-sig\") as infile:\n",
    "    stopwords_en = json.load(infile)[\"en\"]\n",
    "#print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denotations vs global id dictionary\n",
    "span_g_id = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_denotation(denotation, denotation_span):\n",
    "    \n",
    "    token_denotation_info = dict()\n",
    "\n",
    "    token_denotation_info = {\"span\" : denotation_span, \"obj\" : denotation[\"obj\"],\"tag\" : dict()}#, \"relation\" : json_object_relations}\n",
    "    \n",
    "    #getting tag of tokens in denotation_span(disease or gene or expression)\n",
    "    for token in denotation_span:\n",
    "        if str(token) in token_denotation_info[\"tag\"]:\n",
    "            if str(token.tag_) in token_denotation_info[\"tag\"][str(token)]:\n",
    "                token_denotation_info[\"tag\"][str(token)][str(token.tag_)] += 1\n",
    "                ##\n",
    "#                print(token_denotation_info[span_g_id[denotation_span_str]][\"tag\"][token])\n",
    "            else:\n",
    "                token_denotation_info[\"tag\"].update({str(token) : {str(token.tag_) : 1}})\n",
    "        else:\n",
    "            token_denotation_info[\"tag\"].update({str(token) : {str(token.tag_) : 1}})\n",
    "\n",
    "    return token_denotation_info;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_token_denotation_infos(token_denotation_infos, denotation_span):\n",
    "    denotation_span_str = str(denotation_span)\n",
    "    for token in denotation_span:                \n",
    "        if str(token) in token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"]:\n",
    "            if str(token.tag_) in token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"][str(token)]:\n",
    "                token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"][str(token)][str(token.tag_)] += 1\n",
    "                ##\n",
    "#               print(token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"][token])\n",
    "            else:\n",
    "                token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"].update({str(token) : {str(token.tag_) : 1}})\n",
    "        else:\n",
    "            token_denotation_infos[span_g_id[denotation_span_str]][\"tag\"].update({str(token) : {str(token.tag_) : 1}})\n",
    "    return token_denotation_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-1ef7c0a93bb5>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-1ef7c0a93bb5>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    if(pr_w.i-1 >= sent.start)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_sentences_containing_denotations(sentence_with_denotation, text, denotation, no_pre_post):\n",
    "    words = list()\n",
    "    for sent in text.sents:\n",
    "        if(denotation.start >= sent.start and denotation.end < sent.end):\n",
    "            #print(sent)\n",
    "            if(denotation.start != sent.start):\n",
    "                pr_w = text[denotation.start-1]\n",
    "                for i in range(0,no_pre_post):\n",
    "                    if(pr_w.i-1 >= sent.start):\n",
    "                        while(pr_w.string.strip() in stopwords_en):\n",
    "                            pr_w = text[pr_w.i-1]\n",
    "                        words.append(pr_w.string.strip())\n",
    "                        pr_w = text[pr_w.i-1]\n",
    "            \n",
    "            words.append(denotation.string.strip())\n",
    "            \n",
    "            if(denotation.end != sent.end):\n",
    "                ps_w = text[denotation.end]\n",
    "                for i in range(0,no_pre_post):\n",
    "                    if(ps_w.i+1 < sent.end):\n",
    "                        while(ps_w.string.strip() in stopwords_en):\n",
    "                            ps_w = text[ps_w.i+1]\n",
    "                        words.append(ps_w.string.strip())\n",
    "                        ps_w = text[ps_w.i+1]\n",
    "            \n",
    "            if(sent.string.strip() in sentence_with_denotation):\n",
    "                sentence_with_denotation[sent.string.strip()].append(denotation.string.strip())\n",
    "            else:\n",
    "                sentence_with_denotation[sent.string.strip()] =[denotation.string.strip()]\n",
    "    return sentence_with_denotation, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_info(json_objects, no_pre_post):\n",
    "    \n",
    "    token_denotation_infos = dict()\n",
    "    token_relation_infos = dict()\n",
    "    sentences_by_article = dict()\n",
    "    words_by_article = dict()\n",
    "    \n",
    "    #count for gene disease and expression\n",
    "    gene_count = 1\n",
    "    disease_count = 1\n",
    "    expression_count = 1\n",
    "    #regulation_count = 1\n",
    "    \n",
    "    #iterate through the json objects\n",
    "    for jindex,json_object in enumerate(json_objects): \n",
    "        json_object_text = json_object[\"text\"]\n",
    "        json_object_denotations = json_object[\"denotations\"]\n",
    "        json_object_relations = json_object[\"relations\"]\n",
    "        json_object_pubmed_id = json_object[\"sourceid\"]\n",
    "        \n",
    "        ##\n",
    "        #pprint(json_object_relations)\n",
    "        #print()\n",
    "        \n",
    "        #parsing text using spacy\n",
    "        parsed_text = nlp(json_object_text)\n",
    "        \n",
    "        #local and global id dictionary for each json object\n",
    "        lg_id = dict()\n",
    "        sentence_with_denotation = dict()\n",
    "        words_by_article[json_object_pubmed_id] = dict()\n",
    "        \n",
    "        #iterate through denotation within json object\n",
    "        for index,denotation in enumerate(json_object_denotations):\n",
    "            denotation_start = denotation[\"span\"][\"begin\"]\n",
    "            denotation_end = denotation[\"span\"][\"end\"]\n",
    "            denotation_span = parsed_text.char_span(denotation_start, denotation_end)\n",
    "            denotation_span_str = str(denotation_span)\n",
    "            \n",
    "            #print(str(denotation_span.rights))\n",
    "            \n",
    "            sentence_with_denotation, words_by_article[json_object_pubmed_id][denotation_span_str] = get_sentences_containing_denotations(sentence_with_denotation, parsed_text, denotation_span, no_pre_post)\n",
    "            #return\n",
    "            ##\n",
    "#             print(\"[\", jindex, \"]\", denotation, \"-> \", denotation_span, \" : \", denotation_span_str)\n",
    "            \n",
    "            if denotation_span_str in span_g_id:\n",
    "                token_denotation_infos = update_token_denotation_infos(token_denotation_infos, denotation_span)\n",
    "                lg_id[denotation[\"id\"]] = span_g_id[denotation_span_str]\n",
    "            else:\n",
    "                if (\"expression\" in denotation[\"obj\"].lower() or \"regulation\" in denotation[\"obj\"].lower()):\n",
    "                    key = \"e_\" + json_object_pubmed_id + \"_\" + str(expression_count)\n",
    "                    token_denotation_infos[key] = generate_token_denotation(denotation,denotation_span)\n",
    "                    #populating span_g_id\n",
    "                    span_g_id[denotation_span_str] = key\n",
    "                \n",
    "                    #populating lg_id\n",
    "                    lg_id[denotation[\"id\"]] = key\n",
    "                    expression_count += 1\n",
    "\n",
    "                elif (\"cancer\" in denotation[\"obj\"].lower()):\n",
    "                    key = \"d_\" + str(disease_count)\n",
    "                    token_denotation_infos[key] = generate_token_denotation(denotation,denotation_span)\n",
    "                    #populating span_g_id\n",
    "                    span_g_id[denotation_span_str] = key\n",
    "                \n",
    "                    #populating lg_id\n",
    "                    lg_id[denotation[\"id\"]] = key\n",
    "                    disease_count += 1\n",
    "\n",
    "                elif (\"gene\" in denotation[\"obj\"].lower()):\n",
    "                    key = \"g_\" + str(gene_count)\n",
    "                    token_denotation_infos[key] = generate_token_denotation(denotation,denotation_span)\n",
    "                    #populating span_g_id\n",
    "                    span_g_id[denotation_span_str] = key\n",
    "                \n",
    "                    #populating lg_id\n",
    "                    lg_id[denotation[\"id\"]] = key\n",
    "                    gene_count += 1\n",
    "                    \n",
    "                else:\n",
    "                    print(\"EXCEPTION: \", denotation, \"-> \", denotation_span)\n",
    "                    span_g_id[denotation_span_str] = \"EXCEPTION\"\n",
    "                    lg_id[denotation[\"id\"]] = \"EXCEPTION\"\n",
    "        #return\n",
    "        sentences_by_article[json_object_pubmed_id] = sentence_with_denotation\n",
    "        ##\n",
    "        #print(lg_id)\n",
    "        #print()\n",
    "\n",
    "        relations = list()\n",
    "        for index,relation in enumerate(json_object_relations):\n",
    "            relations.append({\"subj\" : lg_id[relation[\"subj\"]], \"obj\" : lg_id[relation[\"obj\"]], \"pred\" : relation[\"pred\"]})\n",
    "        token_relation_infos[json_object_pubmed_id] = relations\n",
    "##\n",
    "#     print(span_g_id)\n",
    "    #pprint(token_denotation_infos)\n",
    "    #print()\n",
    "    #pprint(token_relation_infos)\n",
    "    \n",
    "    return token_denotation_infos, token_relation_infos, sentences_by_article, words_by_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = \"F:/UB/2nd Sem/Basic Projects/Graph Data mining for Gene Cancer relation/CoMAGC-annotations/CoMAGC/\"\n",
    "json_objects = load_json_files(path_to_json)\n",
    "nodes_info, edges_info, sentences, bag_of_words = generate_token_info(json_objects,3)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_labels(node_infos):\n",
    "    nodes = list()\n",
    "    n_labels = dict()\n",
    "    for node,info in node_infos.items():\n",
    "        nodes.append(node)\n",
    "        n_labels[node] = str(info[\"span\"])\n",
    "    return nodes, n_labels\n",
    "\n",
    "def get_edge_labels(edge_infos):\n",
    "    e_labels = dict()\n",
    "    for k,edges in edge_infos.items():\n",
    "        for edge in edges:\n",
    "            e_labels[(edge[\"subj\"],edge[\"obj\"])] = e_labels.get((edge[\"subj\"],edge[\"obj\"]), edge[\"pred\"]) + \"/\" + edge[\"pred\"]\n",
    "    return e_labels\n",
    "\n",
    "def get_edges_labels_weights():\n",
    "    labels = get_edge_labels(edges_info)\n",
    "    e_labels = dict()\n",
    "    edges = list()\n",
    "    for key, value in labels.items():\n",
    "        v_arr = value.split(\"/\")\n",
    "        v_dict = dict()\n",
    "        weight = 0\n",
    "        l = \"\"\n",
    "        for x in v_arr:\n",
    "            v_dict[x] = v_dict.get(x,0) + 1\n",
    "        k_list = list(v_dict.keys())\n",
    "        for k, v in v_dict.items():\n",
    "            if k == k_list[-1]:\n",
    "                l += str(k) + \"*\" + str(v)\n",
    "            else:\n",
    "                l += str(k) + \"*\" + str(v) + \", \"\n",
    "            weight += v\n",
    "        nkey = key + (weight,)\n",
    "        e_labels[key] = l\n",
    "        edges.append(nkey)\n",
    "#         for et, el in e_labels.items():\n",
    "#             edges.append(et)\n",
    "            #e_labels[et] = el\n",
    "    return edges, e_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_size(min_degree, max_degree, nds):\n",
    "    node_sizes = list()\n",
    "    for nd in nds:\n",
    "        node_sizes.append(list(nd))\n",
    "                          \n",
    "    for index,nd in enumerate(node_sizes):\n",
    "        if nd[1] == 0:\n",
    "            node_sizes[index][1] = (((max_degree-min_degree)/500)+200)*0.5\n",
    "        else:\n",
    "            node_sizes[index][1] = (((max_degree-min_degree)/500)+200)*nd[1]\n",
    "                          \n",
    "    return node_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###try outs\n",
    "\n",
    "# print()\n",
    "nodes, n_labels = get_nodes_labels(nodes_info)\n",
    "edges, e_labels = get_edges_labels_weights()\n",
    "\n",
    "# a = [n for n in nodes if \"e_\" in n]\n",
    "# print(a)\n",
    "\n",
    "\n",
    "def initialize_graph(nodes, edges):\n",
    "    g = nx.DiGraph()\n",
    "\n",
    "\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_weighted_edges_from(edges)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def draw_graph (nodes, n_labels, edges, e_labels):\n",
    "    g = initialize_graph(nodes, edges)\n",
    "    n_out_degree = g.degree(nodes)\n",
    "#     n_out_degree = dict(n_out_degree)\n",
    "\n",
    "    min_pair = min(n_out_degree, key = lambda t: t[1])\n",
    "    max_pair = max(n_out_degree, key = lambda t: t[1])\n",
    "###\n",
    "#     print(min_pair)\n",
    "#     print(max_pair)\n",
    "\n",
    "    node_sizes = compute_node_size(min_pair[1], max_pair[1], n_out_degree)\n",
    "    \n",
    "\n",
    "    for component in sorted(nx.weakly_connected_component_subgraphs(g), key = len, reverse = True):\n",
    "    #     print(component.nodes())\n",
    "        if len(component) < 2:\n",
    "            continue\n",
    "        #matplotlib graph size\n",
    "        plt.figure(1, figsize = (64,64))\n",
    "\n",
    "        #networkx layout\n",
    "        pos = nx.spring_layout(component)\n",
    "        #pos = nx.circular_layout(component)\n",
    "\n",
    "        #subgraph nodes and labels\n",
    "        cmp_n_labels = dict((key, value) for key, value in n_labels.items() if key in component.nodes())\n",
    "        cmp_n_colors = list()\n",
    "        for key in component.nodes():\n",
    "            if \"g_\" in key:\n",
    "                value = \"b\"\n",
    "            elif \"d_\" in key:\n",
    "                value = \"r\"\n",
    "            else:\n",
    "                value = \"g\"\n",
    "            cmp_n_colors.append(value)\n",
    "        \n",
    "        nx.draw_networkx_nodes(component, pos, nodelist = component.nodes(), node_color=cmp_n_colors, \n",
    "                               node_size = [v[1] for v in node_sizes if v[0] in component.nodes()], alpha=0.8)\n",
    "        nx.draw_networkx_labels(component, pos, cmp_n_labels, font_size=10)\n",
    "\n",
    "        #subgraph edges and labels\n",
    "        cmp_e_labels = dict((key, value) for key, value in e_labels.items() if key in component.edges())\n",
    "\n",
    "        nx.draw_networkx_edges(component, pos, edgelist=component.edges(), width=2, alpha=0.5, edge_color='r')\n",
    "        nx.draw_networkx_edge_labels(component, pos, edge_labels = cmp_e_labels, font_size=7)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"test.pdf\")\n",
    "#         break\n",
    "        plt.show()\n",
    "        \n",
    "draw_graph(nodes, n_labels, edges, e_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world. Regular expressions are not always the answer.\"\n",
    "partitions = text.partition(\"Hello,\")\n",
    "print(partitions)\n",
    "pref_words = list(filter(None, partitions[0].split(\" \")))\n",
    "print(pref_words)\n",
    "print()\n",
    "if(len(pref_words)>1):\n",
    "    print(\"l1: \",pref_words[len(pref_words)-1])\n",
    "    print(\"l2: \",pref_words[len(pref_words)-2])\n",
    "elif(len(pref_words)>0 and pref_words[len(pref_words)-1]):\n",
    "    print(\"l1: \",pref_words[len(pref_words)-1])\n",
    "    \n",
    "postf_words = list(filter(None, partitions[2].split(\" \")))\n",
    "print(postf_words)\n",
    "print()\n",
    "if(len(postf_words)>1):\n",
    "    print(\"0: \",postf_words[0])\n",
    "    print(\"1: \",postf_words[1])\n",
    "elif(len(postf_words)>0 and postf_words[0]):\n",
    "    print(\"0: \",postf_words[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
